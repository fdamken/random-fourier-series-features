
@article{alemohammadRecurrentNeuralTangent2020,
  title = {The {{Recurrent Neural Tangent Kernel}}},
  author = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
  year = {2020},
  month = dec,
  journal = {arXiv:2006.10246 [cs, stat]},
  eprint = {2006.10246},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs, including how different time steps are weighted by the RNTK to form the output under different initialization parameters and nonlinearity choices, and how inputs of different lengths are treated. The ability to compare inputs of different length is a property of RNTK that should greatly benefit practitioners. We demonstrate via a synthetic and 56 real-world data experiments that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RNEKKI6P/Alemohammad et al. - 2020 - The Recurrent Neural Tangent Kernel.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/8UZE2QE6/2006.html}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher},
  year = {2006},
  month = jan,
  publisher = {{Springer}},
  isbn = {978-0-387-31073-2},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FFBNT6XK/prml-errata-3rd-20110921.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HPQQIX4K/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/QUV2FUE5/prml-web-sol-2009-09-08.pdf}
}

@inproceedings{blundellWeightUncertaintyNeural2015,
  title = {Weight {{Uncertainty}} in {{Neural Network}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = jun,
  pages = {1613--1622},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/LP46YDL6/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf}
}

@inproceedings{buiDeepGaussianProcesses2016,
  title = {Deep {{Gaussian Processes}} for {{Regression}} Using {{Approximate Expectation Propagation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Bui, Thang and {Hernandez-Lobato}, Daniel and {Hernandez-Lobato}, Jose and Li, Yingzhen and Turner, Richard},
  year = {2016},
  month = jun,
  pages = {1472--1481},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. ...},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/E5NY44KQ/Bui et al. - 2016 - Deep Gaussian Processes for Regression using Appro.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/6ZLSD878/bui16.html}
}

@article{coradduMachineLearningApproaches2016,
  title = {Machine Learning Approaches for Improving Condition-Based Maintenance of Naval Propulsion Plants},
  author = {Coraddu, Andrea and Oneto, Luca and Ghio, Aessandro and Savio, Stefano and Anguita, Davide and Figari, Massimo},
  year = {2016},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
  volume = {230},
  number = {1},
  pages = {136--153},
  publisher = {{SAGE Publications Sage UK: London, England}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/8G7HAP4G/Coraddu et al. - 2016 - Machine learning approaches for improving conditio.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/R39FIDDI/1475090214540874.html}
}

@article{cortezModelingWinePreferences2009,
  title = {Modeling Wine Preferences by Data Mining from Physicochemical Properties},
  author = {Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  year = {2009},
  month = nov,
  journal = {Decision Support Systems},
  series = {Smart {{Business Networks}}: {{Concepts}} and {{Empirical Evidence}}},
  volume = {47},
  number = {4},
  pages = {547--553},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2009.05.016},
  abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.},
  langid = {english},
  keywords = {Model selection,Neural networks,Regression,Sensory preferences,Support vector machines,Variable selection},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4JWCNZFM/Cortez et al. - 2009 - Modeling wine preferences by data mining from phys.pdf}
}

@article{daiScalableKernelMethods2015,
  title = {Scalable {{Kernel Methods}} via {{Doubly Stochastic Gradients}}},
  author = {Dai, Bo and Xie, Bo and He, Niao and Liang, Yingyu and Raj, Anant and Balcan, Maria-Florina and Song, Le},
  year = {2015},
  month = sep,
  journal = {arXiv:1407.5599 [cs, stat]},
  eprint = {1407.5599},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random functions associated with the kernel, and then descending using this noisy functional gradient. We show that a function produced by this procedure after \$t\$ iterations converges to the optimal function in the reproducing kernel Hilbert space in rate \$O(1/t)\$, and achieves a generalization performance of \$O(1/\textbackslash sqrt\{t\})\$. This doubly stochasticity also allows us to avoid keeping the support vectors and to implement the algorithm in a small memory footprint, which is linear in number of iterations and independent of data dimension. Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 8 million handwritten digits from MNIST, 2.3 million energy materials from MolecularSpace, and 1 million photos from ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IBHYI2MK/Dai et al. - 2015 - Scalable Kernel Methods via Doubly Stochastic Grad.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/P87H6AVG/1407.html}
}

@inproceedings{damianouDeepGaussianProcesses2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Damianou, Andreas and Lawrence, Neil D.},
  year = {2013},
  month = apr,
  pages = {207--215},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inpu...},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/DJRDHHMW/Damianou and Lawrence - 2013 - Deep Gaussian Processes.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/TUJ493L4/damianou13a.html}
}

@inproceedings{daoGaussianQuadratureKernel2017,
  title = {Gaussian {{Quadrature}} for {{Kernel Features}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dao, Tri and De Sa, Christopher M and R{\'e}, Christopher},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/2UXXLD74/Dao et al. - 2017 - Gaussian Quadrature for Kernel Features.pdf}
}

@misc{duaUCIMachineLearning2017,
  title = {{{UCI Machine Learning Repository}}},
  author = {Dua, Dheeru and Graff, Casey},
  year = {2017},
  publisher = {{University of California, Irvine, School of Information and Computer Sciences}},
  howpublished = {http://archive.ics.uci.edu/ml}
}

@article{dutordoirDeepNeuralNetworks2021,
  title = {Deep {{Neural Networks}} as {{Point Estimates}} for {{Deep Gaussian Processes}}},
  author = {Dutordoir, Vincent and Hensman, James and {van der Wilk}, Mark and Ek, Carl Henrik and Ghahramani, Zoubin and Durrande, Nicolas},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04504 [cs, stat]},
  eprint = {2105.04504},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep Gaussian processes (DGPs) have struggled for relevance in applications due to the challenges and cost associated with Bayesian inference. In this paper we propose a sparse variational approximation for DGPs for which the approximate posterior mean has the same mathematical structure as a Deep Neural Network (DNN). We make the forward pass through a DGP equivalent to a ReLU DNN by finding an interdomain transformation that represents the GP posterior mean as a sum of ReLU basis functions. This unification enables the initialisation and training of the DGP as a neural network, leveraging the well established practice in the deep learning community, and so greatly aiding the inference task. The experiments demonstrate improved accuracy and faster training compared to current DGP methods, while retaining favourable predictive uncertainties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/H49E9T5R/Dutordoir et al. - 2021 - Deep Neural Networks as Point Estimates for Deep G.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/72L7NS92/2105.html}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash{} extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MBJMZ3ZR/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@inproceedings{galDropoutBayesianApproximation2016a,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash{} extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/VXHI7RYB/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@inproceedings{gardnerGPyTorchBlackboxMatrixMatrix2018,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  shorttitle = {{{GPyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n\^3) to O(n\^2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/J8AX7JS5/Gardner et al. - 2018 - GPyTorch Blackbox Matrix-Matrix Gaussian Process .pdf}
}

@article{jacotNeuralTangentKernel2020,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2020},
  month = feb,
  journal = {arXiv:1806.07572 [cs, math, stat]},
  eprint = {1806.07572},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_\textbackslash theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_\textbackslash theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/Y8Z8SA8H/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/IBK6NWNA/1806.html}
}

@inproceedings{kayaLocalGlobalLearning2012,
  title = {Local and Global Learning Methods for Predicting Power of a Combined Gas \& Steam Turbine},
  booktitle = {Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering {{ICETCEE}}},
  author = {Kaya, Heysem and T{\"u}fekci, Pmar and G{\"u}rgen, Fikret S.},
  year = {2012},
  pages = {13--18}
}

@article{kingmaAdamMethodStochastic2017a,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/EZMQ3NZW/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/MU2U6R87/1412.html}
}

@inproceedings{lakshminarayananSimpleScalablePredictive2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem.  Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian)  NNs.  We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/TQJ39TB9/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf}
}

@article{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  year = {2018},
  month = mar,
  journal = {arXiv:1711.00165 [cs, stat]},
  eprint = {1711.00165},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/5VXY6VCJ/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/VYD8BZAH/1711.html}
}

@article{leibfriedTutorialSparseGaussian2021,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2021},
  month = apr,
  journal = {arXiv:2012.13962 [cs, stat]},
  eprint = {2012.13962},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/GDSKS9F5/Leibfried et al. - 2021 - A Tutorial on Sparse Gaussian Processes and Variat.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/DJKEPICV/2012.html}
}

@article{malachQuantifyingBenefitUsing2021,
  title = {Quantifying the {{Benefit}} of {{Using Differentiable Learning}} over {{Tangent Kernels}}},
  author = {Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.01210 [cs, stat]},
  eprint = {2103.01210},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/WILP3HUZ/Malach et al. - 2021 - Quantifying the Benefit of Using Differentiable Le.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/2MH7GJ2L/2103.html}
}

@book{oppenheimSignalsSystems1997,
  title = {{Signals \& Systems}},
  author = {Oppenheim, Alan V. and Willsky, Alan S. and Nawab, Syed Hamid and Hamid, with and Hern{\'a}ndez, Gloria Mata},
  year = {1997},
  publisher = {{Pearson Educaci\'on}},
  abstract = {New edition of a text intended primarily for the undergraduate courses on the subject which are frequently found in electrical engineering curricula--but the concepts and techniques it covers are also of fundamental importance in other engineering disciplines. The book is structured to develop in parallel the methods of analysis for continuous-time and discrete-time signals and systems, thus allowing exploration of their similarities and differences. Discussion of applications is emphasized, and numerous worked examples are included. Annotation copyrighted by Book News, Inc., Portland, OR},
  googlebooks = {g2750K3PxRYC},
  isbn = {978-970-17-0116-4},
  langid = {spanish},
  keywords = {Computers / Software Development \& Engineering / Systems Analysis \& Design}
}

@inproceedings{rahimiRandomFeaturesLargeScale2007,
  title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2007},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/UAKIA8P6/Rahimi and Recht - 2007 - Random Features for Large-Scale Kernel Machines.pdf}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian {{Processes}} for {{Machine Learning}}},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  edition = {Second},
  publisher = {{MIT Press}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QC6GCIVE/Rasmussen and Williams - 2006 - Gaussian Processes for Machine Learning.pdf}
}

@inproceedings{shankarNeuralKernelsTangents2020,
  title = {Neural {{Kernels Without Tangents}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and {Fridovich-Keil}, Sara and {Ragan-Kelley}, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
  year = {2020},
  month = nov,
  pages = {8614--8623},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and momen...},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/5ZKI4NA3/Shankar et al. - 2020 - Neural Kernels Without Tangents.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/KV2YRMHL/shankar20a.html}
}

@book{steinInterpolationSpatialData1999,
  title = {Interpolation of {{Spatial Data}}: {{Some Theory}} for {{Kriging}}},
  shorttitle = {Interpolation of {{Spatial Data}}},
  author = {Stein, Michael L.},
  year = {1999},
  month = jun,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Prediction of a random field based on observations of the random field at some set of locations arises in mining, hydrology, atmospheric sciences, and geography. Kriging, a prediction scheme defined as any prediction scheme that minimizes mean squared prediction error among some class of predictors under a particular model for the field, is commonly used in all these areas of prediction. This book summarizes past work and describes new approaches to thinking about kriging.},
  googlebooks = {5n\_XuL2Wx1EC},
  isbn = {978-0-387-98629-6},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Science / Earth Sciences / General,Science / Earth Sciences / Geography,Science / Earth Sciences / Geology,Technology \& Engineering / Mining}
}

@article{tsanasAccurateQuantitativeEstimation2012,
  title = {Accurate Quantitative Estimation of Energy Performance of Residential Buildings Using Statistical Machine Learning Tools},
  author = {Tsanas, Athanasios and Xifara, Angeliki},
  year = {2012},
  month = jun,
  journal = {Energy and Buildings},
  volume = {49},
  pages = {560--567},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2012.03.003},
  abstract = {We develop a statistical machine learning framework to study the effect of eight input variables (relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution) on two output variables, namely heating load (HL) and cooling load (CL), of residential buildings. We systematically investigate the association strength of each input variable with each of the output variables using a variety of classical and non-parametric statistical analysis tools, in order to identify the most strongly related input variables. Then, we compare a classical linear regression approach against a powerful state of the art nonlinear non-parametric method, random forests, to estimate HL and CL. Extensive simulations on 768 diverse residential buildings show that we can predict HL and CL with low mean absolute error deviations from the ground truth which is established using Ecotect (0.51 and 1.42, respectively). The results of this study support the feasibility of using machine learning tools to estimate building parameters as a convenient and accurate approach, as long as the requested query bears resemblance to the data actually used to train the mathematical model in the first place.},
  langid = {english},
  keywords = {Building energy evaluation,Cooling load,Heating load,Non-parametric statistics,Statistical machine learning},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/GY5WB7XX/S037877881200151X.html}
}

@article{tufekciPredictionFullLoad2014,
  title = {Prediction of Full Load Electrical Power Output of a Base Load Operated Combined Cycle Power Plant Using Machine Learning Methods},
  author = {T{\"u}fekci, P{\i}nar},
  year = {2014},
  month = sep,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {60},
  pages = {126--140},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2014.02.027},
  abstract = {Predicting full load electrical power output of a base load power plant is important in order to maximize the profit from the available megawatt hours. This paper examines and compares some machine learning regression methods to develop a predictive model, which can predict hourly full load electrical power output of a combined cycle power plant. The base load operation of a power plant is influenced by four main parameters, which are used as input variables in the dataset, such as ambient temperature, atmospheric pressure, relative humidity, and exhaust steam pressure. These parameters affect electrical power output, which is considered as the target variable. The dataset, which consists of these input and target variables, was collected over a six-year period. First, based on these variables the best subset of the dataset is explored among all feature subsets in the experiments. Then, the most successful machine learning regression method is sought for predicting full load electrical power output. Thus, the best performance of the best subset, which contains a complete set of input variables, has been observed using the most successful method, which is Bagging algorithm with REPTree, with a mean absolute error of 2.818 and a Root Mean-Squared Error of 3.787.},
  langid = {english},
  keywords = {Combined cycle power plants,Machine learning methods,Prediction of electrical power output},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MWRIQFT7/S0142061514000908.html}
}

@inproceedings{watsonLatentDerivativeBayesian2021,
  title = {Latent {{Derivative Bayesian Last Layer Networks}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Watson, Joe and Lin, Jihao Andreas and Klink, Pascal and Pajarinen, Joni and Peters, Jan},
  year = {2021},
  month = mar,
  pages = {1198--1206},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Bayesian neural networks (BNN) are powerful parametric models for nonlinear regression with uncertainty quantification. However, the approximate inference techniques for weight space priors suffer from several drawbacks. The `Bayesian last layer' (BLL) is an alternative BNN approach that learns the feature space for an exact Bayesian linear model with explicit predictive distributions. However, its predictions outside of the data distribution (OOD) are typically overconfident, as the marginal likelihood objective results in a learned feature space that overfits to the data. We overcome this weakness by introducing a functional prior on the model's derivatives w.r.t. the inputs. Treating these Jacobians as latent variables, we incorporate the prior into the objective to influence the smoothness and diversity of the features, which enables greater predictive uncertainty. For the BLL, the Jacobians can be computed directly using forward mode automatic differentiation, and the distribution over Jacobians may be obtained in closed-form. We demonstrate this method enhances the BLL to Gaussian process-like performance on tasks where calibrated uncertainty is critical: OOD regression, Bayesian optimization and active learning, which include high-dimensional real-world datasets.},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/7PLNXH28/Watson et al. - 2021 - Latent Derivative Bayesian Last Layer Networks.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/ZSEQ4TPJ/Watson et al. - 2021 - Latent Derivative Bayesian Last Layer Networks.pdf}
}

@article{yehModelingStrengthHighperformance1998,
  title = {Modeling of Strength of High-Performance Concrete Using Artificial Neural Networks},
  author = {Yeh, I. -C.},
  year = {1998},
  month = dec,
  journal = {Cement and Concrete Research},
  volume = {28},
  number = {12},
  pages = {1797--1808},
  issn = {0008-8846},
  doi = {10.1016/S0008-8846(98)00165-3},
  abstract = {Several studies independently have shown that concrete strength development is determined not only by the water-to-cement ratio, but that it also is influenced by the content of other concrete ingredients. High-performance concrete is a highly complex material, which makes modeling its behavior a very difficult task. This paper is aimed at demonstrating the possibilities of adapting artificial neural networks (ANN) to predict the compressive strength of high-performance concrete. A set of trial batches of HPC was produced in the laboratory and demonstrated satisfactory experimental results. This study led to the following conclusions: 1) A strength model based on ANN is more accurate than a model based on regression analysis; and 2) It is convenient and easy to use ANN models for numerical experiments to review the effects of the proportions of each variable on the concrete mix.},
  langid = {english},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/YTC8VNSK/S0008884698001653.html}
}


