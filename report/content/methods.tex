In this section we cover the methodology and core contributions of our work, staring with a discussion of \acp{RFF} and introducing the novel \acp{RFSF}.

\subsection{Random Fourier Features}
	By explicitly modeling the features, the inversion of the Gram matrix can be avoided by resorting to computing the distribution over the weights explicitly, i.e., "switching back" to Bayesian linear regression.
	A well-known approach for this are \acp{RFF}\cite{rahimiRandomFeaturesLargeScale2007} which approximate the \ac{SE} kernel.
	A single \ac{RFF} is given by
	\begin{equation}
		\vec{z}_{\vec{\omega}}(\vec{x}) =
			\begin{bmatrix}
				\cos(\ip{\vec{\omega}}{\vec{x}}) \\
				\sin(\ip{\vec{\omega}}{\vec{x}})
			\end{bmatrix}\!.
		\label{eq:rffFeature}
	\end{equation}
	This definition is based on the observation that
	\begin{equation}
		k(\vec{x} - \vec{y}) = \E_{\vec{\omega} \sim p(\cdot)}\bigl[ \ip{\vec{z}_{\vec{\omega}}(\vec{x})}{\vec{z}_{\vec{\omega}}(\vec{y})} \bigr]
		\label{eq:rffExpectation}
	\end{equation}
	where $k(\vec{x} - \vec{y})$ is a stationary kernel and $p(\vec{\omega})$ is its Fourier transform (which is a proper probability distribution due to Bochner's theorem\cite{steinInterpolationSpatialData1999}).
	For the \ac{SE} kernel, $p(\vec{\omega})$ is tractable and equal to a standard normal distribution\cite{rasmussenGaussianProcessesMachine2006}.
	As the integral in \cref{eq:rffExpectation} is intractable, it is usually approximated using Monte-Carlo estimation over $\vec{\omega}$.
	Let $\{ \vec{w}_j \}_{j = 1}^{D}$ be the particles sampled from $p(\vec{\omega})$.
	The corresponding feature particles $\vec{z}_{\vec{\omega}_j}(\cdot)$ are then concatenated, forming the complete feature $\vec{z}(\cdot)$ which is scaled by $1/\sqrt{D}$ such that the inner product yields the usual Monte Carlo approximation of an expectation.
% end

\subsection{Random Fourier Series Features}
	We extend \acp{RFF} to random Fourier \emph{series} features (\acsu{RFSF}) by splitting up \cref{eq:rffFeature} further into sub-features with separate amplitudes for the sine/cosine component and adding the respective scaling factors for $\vec{x}$:
	\begin{equation}
		\tilde{\vec{z}}_{\vec{\omega}}^{\,(k)}(\vec{x}) =
			\begin{bmatrix}
				a_k \cos(\omega k \mel{\vec{\omega}}{\mat{\Lambda}^{-1}}{\vec{x}}) \\
				b_k \sin(\omega k \mel{\vec{\omega}}{\mat{\Lambda}^{-1}}{\vec{x}})
			\end{bmatrix}\!.
		\label{eq:rfsf}
	\end{equation}
	which are then summed over $k = 1, 2, \dots, K$.
	The matrix $\mat{\Lambda}$ represents the length-scales which can be either isotropic for a single length-scale but also different for the input dimensions, allowing \ac{ARD}.
	This formulation is inspired by the sine-cosine formulation of a Fourier series (\cref{eq:sineCosineFourierSeries}), motivating the name.
	Note that the half-period $\tilde{T}$ has the function of a "secondary" length-scale applied equally to all input dimensions.

	To find the optimal hyper-parameters $\vec{a}_{0:M}$, $\vec{b}_{0:M}$, and $\mat{\Lambda}$, we use the empirical Bayes approximation\cite[p.\,165]{bishopPatternRecognitionMachine2006}, i.e., maximize the marginal log-likelihood over the training data.
	Beyond the kernel's parameters, the data is assumed to have Gaussian aleatoric noise with zero mean and variance $\sigma_n^2$ which is an additional hyper-parameter.

	As said in the introduction, theoretical analysis is up to future work.
	We will therefore directly move to empirical evaluation in the next section.
% end


%\begin{figure*}
%	\begin{align}
%		\tilde{k}(\vec{x}, \vec{y})
%			&= \ip{\tilde{\vec{z}}_{\vec{\omega}}(\vec{x})}{\tilde{\vec{z}}_{\vec{\omega}}(\vec{y})} \\
%			&= \ip{\sum_{m = 1}^{M} \tilde{\vec{z}}_{\vec{\omega}}^{\,(m)}(\vec{x})}{\sum_{m = 1}^{M} \tilde{\vec{z}}_{\vec{\omega}}^{\,(m)}(\vec{y})} \\
%			&= \ip{\sum_{m = 1}^{M}  \begin{bmatrix} a_m \cos(x_m) \\ b_m \sin(x_m) \end{bmatrix}}{\sum_{m = 1}^{M} \begin{bmatrix} a_m \cos(y_m) \\ b_m \sin(y_m) \end{bmatrix}} \\
%			&= \ip{\begin{bmatrix} \sum_{m = 1}^{M} a_m \cos(x_m) \\ \sum_{m = 1}^{M} b_m \sin(x_m) \end{bmatrix}}{\begin{bmatrix} \sum_{m = 1}^{M} a_m \cos(y_m) \\ \sum_{m = 1}^{M} b_m \sin(y_m) \end{bmatrix}} \\
%			&= \Biggl(\, \sum_{m = 1}^{M} a_m \cos(x_m) \Biggr) \Biggl(\, \sum_{m = 1}^{M} a_m \cos(y_m) \Biggr) + \Biggl(\, \sum_{m = 1}^{M} b_m \sin(x_m) \Biggr) \Biggl(\, \sum_{m = 1}^{M} b_m \sin(y_m) \Biggr) \\
%			&= \sum_{m = 1}^{M} \sum_{n = 1}^{M} a_m a_n \cos(x_m) \cos(y_n) + \sum_{m = 1}^{M} \sum_{n = 1}^{M} b_m b_n \sin(x_m) \sin(y_n) \\
%			&= \sum_{m = 1}^{M} \sum_{n = 1}^{M} a_m a_n \cos(x_m) \cos(y_n) + b_m b_n \sin(x_m) \sin(y_n) \\
%			&= \frac{1}{2} \sum_{m = 1}^{M} \sum_{n = 1}^{M} (a_m a_n + b_m b_n) \cos(x_m - y_n) + (a_m a_n - b_m b_n) \cos(x_m + y_n) \\
%	\end{align}
%
%	\begin{align}
%		\E_{\vec{\omega}}\Bigl[ \tilde{k}(\vec{x}, \vec{y}) \Bigr]
%			&= \frac{1}{2} \sum_{m = 1}^{M} \sum_{n = 1}^{M} (a_m a_n + b_m b_n) \E_{\vec{\omega}}\bigl[ \cos(x_m - y_n) \bigr] + (a_m a_n - b_m b_n) \E_{\vec{\omega}}\bigl[ \cos(x_m + y_n) \bigr] \\
%			&= \frac{1}{2} \sum_{m = 1}^{M} \sum_{n = 1}^{M} (a_m a_n + b_m b_n) \, k_\mathrm{SE}\bigl( m \vec{x} - n \vec{y}; \pi^{-1} \tilde{T} \ell^2 \bigr) + (a_m a_n - b_m b_n) \, k_\mathrm{SE}\bigl( m \vec{x} - (-n \vec{y}); \pi^{-1} \tilde{T} \ell^2 \bigr) \\
%	\end{align}
%
%	\begin{equation}
%		\E_{\vec{\omega}}\bigl[ \cos(x_m - y_n) \bigr]
%			= \E_{\vec{\omega}}\biggl[ \cos\biggl( \frac{\pi}{\tilde{T} \ell^2} \ip{\vec{\omega}}{m \vec{x} - n \vec{y}} \biggr) \biggr]
%			= k_\mathrm{SE}\bigl( m \vec{x} - n \vec{y}; \pi^{-1} \tilde{T} \ell^2 \bigr)
%	\end{equation}
%	\begin{equation}
%		\E_{\vec{\omega}}\bigl[ \cos(x_m + y_n) \bigr]
%		= k_\mathrm{SE}\bigl( m \vec{x} - (-n \vec{y}); \pi^{-1} \tilde{T} \ell^2 \bigr)
%	\end{equation}
%
%	\begin{align}
%		x_m \pm y_n &= \frac{\pi}{\tilde{T} \ell^2} \ip{\vec{\omega}}{m \vec{x} \pm n \vec{y}}
%	\end{align}
%
%	\caption{Some derivations that are likely to not make sense.}
%\end{figure*}
